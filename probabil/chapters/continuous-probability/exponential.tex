%<*probabil-2025-12-05-rozklad-wykladniczy-1>
\subsection{Właściwości}
\begin{definition}
	\textbf{Rozkładem wykładniczym} z parametrem \( \lambda \) nazywamy rozkład zadany gęstością
	
	\begin{minipage}[c]{0.5\linewidth}
		\[
			f(x) = \begin{cases}
				\lambda e^{-\lambda x} & \text{ dla } x \geq 0 \\
				0 					   & \text{wpp.} \\
			\end{cases}
		\]
	\end{minipage}
	\quad
	\begin{minipage}[c]{0.4\linewidth}
		\centering
		\begin{tikzpicture}
		\draw[->] (-0.5, 0) -- (5, 0) node[right] {$x$};
		\draw[->] (0, -0.25) -- (0, 2.2) node[above] {$y$};
		\draw[yscale=3, domain=-0.4:0, smooth, variable=\x, blue] plot ({\x}, {0});
		\draw[yscale=3, domain=0:5, smooth, variable=\x, blue] plot ({\x}, {e^(-\x)});
	\end{tikzpicture}
	\end{minipage}
\end{definition}

Intuicyjnie widzimy, że im \(\lambda\) jest mniejsze, tym bardziej ten wykres się "wypłaszcza".

Dystrybuanta takiej zmiennej wynosi

\begin{minipage}[c]{0.5\linewidth}
	\[
		F(x) = \begin{cases}
			1 - e^{-\lambda x}  & \text{ dla } x \geq 0           \\
			0 					& \text{ wpp. }
		\end{cases}
	\]
\end{minipage}
\quad
\begin{minipage}[c]{0.4\linewidth}
	\centering
	\begin{tikzpicture}
	\draw[->] (-0.5, 0) -- (5, 0) node[right] {$x$};
	\draw[->] (0, -0.25) -- (0, 2.2) node[above] {$y$};
	\draw[yscale=3, domain=-0.4:0, smooth, variable=\x, blue] plot ({\x}, {0});
	\draw[yscale=3, domain=0:5, smooth, variable=\x, blue] plot ({\x}, {1 - e^(-\x)});
\end{tikzpicture}
\end{minipage}

dodatkowo definiujemy
\[
	G(x) = P(X > x) = 1 - F(x) = \begin{cases}
		e^{-\lambda x}  & \text{ dla } x \geq 0           \\
		0 					& \text{ wpp. }
	\end{cases}
\]

\begin{lemma}
	\label{exponential-divided-by-constant}
	Dla \(X \sim \Exp(a)\) oraz \(Y = \frac{X}{b}\) zachodzi
	\[
		Y \sim \Exp(ab)
	\]
\end{lemma}
\begin{proof}
	\[
		\prob(Y \leq y) = \prob(X \leq by) = 1 - e^{-aby}
	\]
\end{proof}

\begin{theorem}
	Dla \(X \sim \Exp(\lambda)\) zachodzi
	\[
		\expected{X} = \frac{1}{\lambda}
	\]
	\[
		\expected{X^2} = \frac{2}{\lambda^2}
	\]
	\[
		\variance{X} =\frac{1}{\lambda^2}
	\]
\end{theorem}
\begin{proof}
	\begin{align*}
		\ev{X} &= \int_{-\infty}^\infty t f(t) \diff t\\
		&= \int_0^\infty t \lambda e^{-\lambda t} \diff t\\
		&= -\int_0^\infty t \pars{-\lambda e^{-\lambda t}} \diff t\\
		&= -\brackets{t e^{-\lambda t}}_0^\infty + \int_0^\infty e^{-\lambda t} \diff t\\
		&= 0 + \brackets{-\frac{1}{\lambda} e^{-\lambda t}}_0^\infty\\
		&= \frac{1}{\lambda}
	\end{align*}
	
	\begin{align*}
		\ev{X^2} &= \int_0^\infty t^2 \lambda e^{-\lambda t} \diff t\\
		&= -\int_0^\infty t^2 \pars{-\lambda e^{-\lambda t}} \diff t\\
		&= -\brackets{t^2 e^{-\lambda t}}_0^\infty + \int_0^\infty 2t e^{-\lambda t} \diff t\\
		&= 0 + \frac{2}{\lambda} \int_0^\infty t \lambda e^{-\lambda t} \diff t\\
		&= 0 + \frac{2}{\lambda} \cdot \frac{1}{\lambda}\\
		&= \frac{2}{\lambda^2}
	\end{align*}
	
	\[
		\variance{X} = \expected{X^2} - \expected{X}^2 = \frac{1}{\lambda^2}
	\]
\end{proof}
Alternatywnie, dla uproszczenia można to udowodnić wyłącznie dla \(\lambda = 1\) a potem z \ref{exponential-divided-by-constant} rozszerzyć na dowolne \(\lambda\).

\begin{theorem}[Lemat 8.4 P\&C]
	Rozkład wykładniczy jest \textbf{bez pamięci}, tzn. dla \(X \sim \Exp(\lambda)\), \(s, t \in \real^+\) zachodzi
	\[
		P(X > s + t \mid X > t) = P(X > s)
	\]
\end{theorem}
\begin{proof}
	\begin{align*}
		P(X > s + t \mid X > t) & = \frac{P(X > s + t)}{P(X > t)}                  \\
		                        & = \frac{1 - P(X \leq s + t)}{1 - P(X \leq t)}    \\
		                        & = \frac{\exp(-\lambda(s + t))}{\exp(-\lambda t)} \\
		                        & = e^{-\lambda s} = P(X > s)
	\end{align*}
\end{proof}

Jest to bardzo przydatna własność, bowiem sprawia, że możemy ,,resetować'' zmienną o której wiemy, że ma większą wartość niż ustalona.

\begin{theorem}
	Rozkład wykładniczy jest jedynym ciągłym rozkładem bez pamięci o dziedzinie \([0, \infty)\), czyli jeśli \(X\) jest ciągłą zmienną losową i zachodzi
	\[ \forall_{s,t>0} \ P\left( X> s+t \mid X>t \right) = P\left( X >s \right) ,  \]
	to istnieje takie \(\lambda > 0\), że \(X \sim \Exp\left( \lambda \right) \).
\end{theorem}
%</probabil-2025-12-05-rozklad-wykladniczy-1>
\begin{proof}
	Niech \(S\left( x  \right) = 1 - F_X\left( x  \right) \). Mamy
	\[ \frac{S\left( s+t \right) }{S\left( t \right) } = S\left( s  \right) \implies  S\left( s+t \right) = S\left( s  \right) S\left( t  \right) . \]
	Z tego widzimy \(S \left( 2t \right) = S\left( t \right)^2 \) i indukując mamy \(S\left( p t  \right) = S\left( t  \right) ^{p}\), \( S\left( \frac{1}{q}t \right) = S\left( t  \right) ^{\frac{1}{q}} \), czyli \(S\left( \frac{p}{q}t \right) = S\left( t  \right) ^{\frac{p}{q}}\), a więc \(\forall_{r \in \Q_{\ge 0}} \ S\left( rt \right) = S\left( t  \right) ^{r} \), teraz dla \(a\in \R_{\ge 0}\) znajdujemy ciąg \(\left\{ r_n \right\} \) zbiegający do \(a\) i mamy \(S\left( r_n t  \right) = S\left( t  \right) ^{r_n}\), a więc z ciągłości \(S\left( at  \right) = S\left( t  \right) ^{a}\).

	Wstawiając \(t=1\) mamy \(S\left( a  \right) = S\left( 1 \right) ^{a}\). Definiujemy \(\lambda = -\ln S\left( 1 \right) \). Dla \(x \in \R_{\ge 0}\) mamy \(S\left( x  \right) = S\left( 1 \right) ^{x} = e^{\ln S\left( 1 \right) \cdot  x } = e^{-\lambda x}\), czyli rozkład wykładniczy.

	\(S\left( x  \right) \) jest nierosnąca i mamy \( \lim_{x \to 0^{+}} S\left( x  \right) = 1\), więc \(P\left( x < 0 \right) = 0\), czyli dla liczb ujemnych też się zgadza.
\end{proof}


\begin{theorem}[MGF]
	Niech \( X \) ma rozkład wykładniczy z parametrem \( \lambda \). Wtedy dla \( t < \lambda \)
	\[
		M_X(t) = \frac{\lambda}{\lambda - t}
	\]
\end{theorem}
\begin{proof}
	\begin{align*}
		M_X(t)
		 & = \expected{e^{tX}}                                         \\
		 & = \int_0^\infty e^{tx} \cdot f(x) \diff x                   \\
		 & = \int_0^\infty e^{tx} \cdot \lambda e^{-\lambda x} \diff x \\
		 & = \lambda \int_0^\infty e^{-x \cdot (\lambda - t)} \diff x  \\
		 & = \frac{\lambda}{\lambda - t}
	\end{align*}
\end{proof}

%<*probabil-2025-12-05-rozklad-wykladniczy-2>
\begin{theorem}[Lemat 8.5 P\&C]
	\label{lemat8.5}
	Jeśli \(X_1, \dots, X_n\) są \textbf{niezależnymi} zmiennymi losowymi spełniającymi \(X_i \sim \Exp(\lambda_i)\), to 
	\[
		\min(X_1, \dots, X_n) \sim \Exp\pars{\sum_{i=1}^n \lambda_i}
	\]
	oraz
	\[
		\prob(X_i = \min(X_1, \dots, X_n)) = \frac{\lambda_i}{\sum_{j=1}^n \lambda_j}
	\]
\end{theorem}

\begin{proof}
	Przeprowadzimy dowód dla \(n = 2\), który później prostą indukcją można rozszerzyć na \(n > 2\).
	
	\begin{align*}
		\prob(\min(X_1, X_2) > x) &= \prob(X_1 > x \land X_2 > x)\\
		&= \prob(X_1 > x) \cdot \prob(X_2 > x)\\
		&= e^{-\lambda_1 x} \cdot e^{-\lambda_2 x}\\
		&= e^{-(\lambda_1 + \lambda_2)x}
	\end{align*}

	a więc \(\min(X_1, X_2) \sim \Exp(\lambda_1 + \lambda_2)\). Teraz pozostaje pokazać, że:

	\[
		\prob(X_1 \leq X_2) = \frac{\lambda_1}{\lambda_1 + \lambda_2}
	\]

	Zatem liczymy:

	{
	% Wyjątkowo pozwalam na pagebreak wewnątrz align,
	% bo ten align jest ogromny i inaczej formatowanie poprzedniej strony robi się dziwne
	\allowdisplaybreaks
	\begin{align*}
		\mathrm{P}(X_1 \leq X_2) = \int_{x_2 = -\infty}^{\infty} \int_{x_1 = -\infty}^{x_2} f_{X_{1}X_{2}}(x_1, x_2) \; dx_{1}dx_{2}                                                                       & = \\
		\int_{x_2 = -\infty}^{\infty} f_{X_{2}}(x_2) \int_{x_1 = -\infty}^{x_2} f_{X_1}(x_1) \; dx_{1}dx_{2}                                                                                               & = \\
		\int_{x_2 = 0}^{\infty} \lambda_2 e^{-\lambda_{2}x_2} \int_{x_1 = 0}^{x_2} \lambda_1 e^{-\lambda_1 x_1} \; dx_{1}dx_{2}                                                                            & = \\
		\lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_{2}x_2} \int_{x_1 = 0}^{x_2} e^{-\lambda_1 x_1} \; dx_{1}dx_{2}                                                                            & = \\
		\lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_{2}x_2} \pars{\Big\rvert_{0}^{x_2} \frac{-1}{\lambda_1} e^{-\lambda_1 x_1} } \;  dx_{2}                                                    & = \\
		\lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_{2}x_2} \pars{\frac{-1}{\lambda_1} e^{-\lambda_1 x_2} - \frac{-1}{\lambda_1} e^0} \; dx_2                                                  & = \\
		\lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_2 x_2} \frac{-1}{\lambda_1} \pars{e^{-\lambda_1 x_2} - 1} \; dx_2                                                                          & = \\
		-\lambda_2 \int_{x_2=0}^{\infty} e^{-\lambda_2 x_2} \pars{e^{-\lambda_1 x_2} - 1} \; dx_2                                                                                                          & = \\
		-\lambda_2 \int_{x_2=0}^{\infty} e^{-\lambda_2x_2 - \lambda_1 x_2} - e^{-\lambda_2 x_2} \; dx_2                                                                                                    & = \\
		-\lambda_2 \int_{x_2=0}^{\infty} e^{-x_2\pars{\lambda_2 + \lambda_1}} - e^{-\lambda_2 x_2} \; dx_2                                                                                                 & = \\
		-\lambda_2 \pars{\int_{x_2=0}^{\infty} e^{-x_2\pars{\lambda_2 + \lambda_1}} \; dx_2 - \int_{x_2=0}^{\infty} e^{-\lambda_2 x_2} \; dx_2}                                                            & = \\
		-\lambda_2 \pars{\pars{\Big\rvert_{x_2=0}^{\infty} \frac{-1}{\lambda_1 + \lambda_2} e^{-x_2(\lambda_1 + \lambda_2)}} - \pars{\Big\rvert_{x_2=0}^{\infty} \frac{-1}{\lambda_2} e^{-\lambda_2 x_2}}} & = \\
		-\lambda_2 \pars{\frac{1}{\lambda_1 + \lambda_2} - \frac{1}{\lambda_2}}                                                                                                                            & = \\
		-\lambda_2 \pars{\frac{\lambda_2}{\lambda_2 \cdot \pars{\lambda_1 + \lambda_2}} - \frac{\lambda_1 + \lambda_2}{\lambda_2 \cdot \pars{\lambda_1 + \lambda_2}}}                                      & = \\
		(-\lambda_2)\cdot \frac{-\lambda_1}{\lambda_2 \cdot \pars{\lambda_1 + \lambda_2}}                                                                                                                  & = \\
		\frac{\lambda_1}{\lambda_1 + \lambda_2}
	\end{align*}
	}
\end{proof}

\subsection{Kantory}
Jesteśmy na lotnisku, na którym stoi \(n\) kantorów obsługujących klientów. W chwili \(t\) kiedy do nich podchodzimy, wszystkie z nich są zajęte. Przyjmijmy, że czas obsługi przez \(i\)-ty kantor ma rozkład \(X_i \sim \Exp(1)\). Jaki jest rozkład czasu oczekiwania na zwolnienie pierwszego kantoru?

W chwili w której podchodzimy do kantorów, każdy z nich jest zajęty od nieznanej nam chwili wcześniejszej od \(t\). Wiemy jednak, że wszystkie z nich są obecnie zajęte. W takim razie, z faktu, że rozkład wykładniczy jest bez pamięci, możemy myśleć o nich jako o rozkładach wykładniczych rozpoczynających się w momencie \(t\). A więc z \ref{lemat8.5}, rozkład czasu po którym pierwszy kantor się zwolni to po prostu \(\Exp(n)\).
%</probabil-2025-12-05-rozklad-wykladniczy-2>

\subsection{Kule i urny z feedbackiem}

Jak zwykle, zanim zaczniemy to pokażemy pomocniczy lemat:
\begin{lemma}
	\label{if-expected-is-finite-then-variable-is-finite}
	Niech \( X \) będzie dowolną zmienną losową ze skończoną wartością oczekiwaną, tj. \( \expected{X} \in \real \).
	Wtedy
	\[
		P(X < \infty) = 1
	\]
\end{lemma}
\begin{proof}
	Korzystamy z nierówności Markowa
	\[
		P(X \geq n) \leq \frac{\expected{X}}{n}
	\]
	Zatem
	\[
		\lim_{n \rightarrow \infty} P(X \geq n) \leq \lim_{n \rightarrow \infty} \frac{\expected{X}}{n} = 0
	\]
\end{proof}


Kule i urny jakie są każdy widzi. Rozważmy sobie jednak zabawny model, w którym mamy tylko dwie urny ale z takim twistem, że im więcej kul jest w urnie, tym większa szansa na to, że wrzucimy tam kolejną kulę.

Konkretniej - jeśli w pierwszej urnie jest \( x \) kul a w drugiej \( y \) to prawdopodobieństwo, że kolejna kula trafi do pierwszej urny wynosi \[ \frac{x^p}{x^p + y^p} \] a do drugiej \[ \frac{y^p}{x^p + y^p} \]
dla ustalonego \( p \).

Będziemy się zajmować \( p > 1 \) tzn. więcej kul dostaje cięższa urna.

\begin{theorem}
	Dla dowolnego \( p > 1 \) oraz dowolnych warunków początkowych, z prawdopodobieństwem 1 od pewnego momentu kule wpadają tylko do jednej urny.
\end{theorem}
\begin{proof}
	Przyjmijmy, że w obu urnach na początku jest po jednej kuli, uprości to dowód, a rozumowanie pozostaje takie same.

	Rozważmy inny, choć podobny, proces.
	Każda urna dostaje własny, niezależny licznik, który odlicza czas do przyjścia kolejnej kuli do tej konkretnej urny.

	Jeśli w pierwszej urnie jest \( x \) kul to czas oczekiwania na kolejną wynosi \( T_x \), które ma rozkład wykładniczy z parametrem \( x^p \).

	Podobnie dla drugiej urny -- jeśli jest w niej \( y \) kul to mamy zmienną \( U_y \) z parametrem \( y^p \).

	Zauważamy teraz fajną rzecz -- prawdopodobieństwo, że kolejna kula ląduje w pierwszej urnie wynosi dokładnie
	\[
		\frac{x^p}{x^p + y^p}
	\]
	a w drugiej
	\[
		\frac{y^p}{x^p + y^p}
	\]
	Czyli nasz nowy proces jest taki sam jak oryginalny, cóż za zbieg okoliczności.

	Definiujemy czasy nasycenia -- opisują one po jakim czasie liczba kul w urnach jest dowolnie duża.
	\[
		F_1 = \sum_{i=1}^\infty T_i
	\]
	\[
		F_2 = \sum_{i=1}^\infty U_i
	\]
	Możemy tak zrobić, bo \(\expected{T_i} = \expected{U_i} = \frac{1}{i^p}\), a ponieważ \( p > 1 \) to \( \expected{F_1}\) oraz \(\expected{F_2} \) są skończone.

	Tutaj należy uważać ale książka Wam tego nie powie.
	Otóż a priori nie wiemy, że jeśli zmienna ma skończoną oczekiwaną to z prawdopodobieństwem 1 \textit{zmienna} przyjmuje skończoną wartość.
	My się powołujemy na lemat \ref{if-expected-is-finite-then-variable-is-finite}
	dzięki czemu wiemy, że wartości \( F_1, F_2 \) są skończone.


	Co więcej, z prawdopodobieństwem 1 są różne.

	Bez straty ogólności, przyjmijmy, że \( F_2 > F_1 \). Oznacza to, że dla pewnego \( n \)
	\[
		\sum_{i=1}^n U_i < F_1 < \sum_{i=1}^{n+1} U_i
	\]
	a to z kolei oznacza, że dla wystarczająco dużych \( m \)
	\[
		\sum_{i=1}^n U_i < \sum_{i=1}^m T_i < \sum_{i=1}^{n+1} U_i
	\]
	W takim razie, dla odpowiednio dużych \( m \) pierwsza urna zawiera \( m \) kul a druga urna zawiera jedynie \( n \) kul, czyli z prawdopodobieństwem 1 druga urna utknęła na posiadaniu \( n \) kul, a to jest to co chcieliśmy pokazać.
\end{proof}

\subsection{Ćwiczenia}

\begin{exercise}
	Rozważ zmienną o rozkładzie wykładniczym z parametrem \(\lambda\).
	Jaki rozkład ma zmienna \(\ceil{X}\)? Czy ten rozkład coś Ci przypomina?
\end{exercise}
\begin{proof}
	Niech \( F \) będzie dystrybuantą \( X \), \( F(x) = 1 - e^{-\lambda x} \) dla \( x \geq 0 \).

	Wtedy
	\begin{align*}
		P\pars{\ceil{X} = n}
		 & = F(n) - F(n - 1)                                         \\
		 & = e^{-\lambda(n-1)} - e^{-\lambda n}                      \\
		 & = \pars{e^{-\lambda}}^{n-1} \cdot \pars{1 - e^{-\lambda}}
	\end{align*}

	Jeśli przyjmiemy \( p = 1 - e^{-\lambda} = F(1) \) to dostaniemy \( P\pars{\ceil{X} = n} = (1-p)^{n-1} \cdot p \)

	Rozkład geometryczny z parametrem \(F(1)\). Wow.

\end{proof}
