%<*probabil-2025-10-24-nierownosc-chernoffa-1>
%<*probabil-egzamin-chernoff-1>
\subsection{Definicja}
Łączymy ze sobą dwie rzeczy -- funkcje tworzące momenty, oraz nierówność Markowa.
\begin{theorem}
	\label{chernoff-inequality}
	\[
		\forall_{t > 0} : P(X \geq a) = P(e^{tX} \geq e^{ta}) \leq \frac{\expected{e^{tX}}}{e^{ta}}
	\]
	oraz
	\[
		\forall_{t < 0} : P(X \leq a) = P(e^{tX} \geq e^{ta}) \leq \frac{\expected{e^{tX}}}{e^{ta}}
	\]
    w szczególności
    \[
        \prob(X \geq a) \leq min_{t > 0}\left\{ \frac{\expected{e^{tx}}}{e^{ta}} \right\}
    \]
\end{theorem}
\begin{proof}
	Niezależnie od tego jakie wartości przyjmuje \( X \) oraz ile wynosi \( t \) to \( e^{tX} \) oraz \( e^{ta} \) zawsze będą dodatnie.
	Monotoniczność \( e^{tx} \) przy ustalonym \( t \) zależy jedynie od znaku zatem przejścia między prawdopodobieństwami zachodzą.

	Ograniczenie górne uzyskujemy korzystając z nierówności Markowa zastosowanej do (dodatnich) wartości \( e^{tX} \) oraz \( e^{ta} \).
\end{proof}

\subsection{Próby Poissona}
\begin{definition}
	\textbf{Próbami Poissona} nazywany ciąg zmiennych losowych \( X_1, \dots, X_n \), dla których
	\[
		P(X_i = 1) = p_i \land P(X_i = 0) = 1 - p_i
	\]
	Ponadto definiujemy
	\[
		\mu = \expected{\sum X_i} = \sum \expected{X_i} = \sum p_i
	\]
    Dodatkowo, jeżeli \( \forall_{i,j \in [n]} p_i = p_j \), to nazywamy to \textbf{próbami Bernoulliego}.
\end{definition}
%</probabil-2025-10-24-nierownosc-chernoffa-1>

%<*probabil-2025-11-14-poisson-trial-chernoff-bound>
\begin{theorem}
	\label{poisson-trial-chernoff-bounds}
	Niech \(X_1, \dots, X_n\) to niezależne próby Poissona. Dodatkowo oznaczamy \(X = \sum_{i=1}^n{X_i}\) i \(\mu = \expected{X}\). Wtedy
	\begin{enumerate}
		\item jeśli \(\delta > 0\), to \(\prob \pars{X \ge \pars{1+\delta}\mu} \le \pars{\frac{e^{\delta}}{ \pars{1+\delta}^{1+\delta}}} ^{\mu}\)   \\
		\item jeśli \(1 \geq \delta > 0\), to \(\prob \pars{X \ge \pars{1+\delta}\mu} \le e^{\frac{-\mu\delta^2}{3}}\)                                 \\
		\item jeśli \(R \ge 6\mu\), to \(\prob \pars{X \ge R} \le 2^{-R}\)
	\end{enumerate}
\end{theorem}
\begin{proof}
	Liczymy funkcję tworzącą \[M_{X_i} \pars{ t } = \mathbb{E}\left[ e ^{tX_i} \right] = p_ie^{t}+ \pars{ 1-p_i } = 1 + p_i \pars{ e^{t}-1 } \le e^{p_i \pars{ e^{t}-1 } } .\]

	Zatem \[M_X \pars{ t } = \prod_{i=1}^{n} M_{X_i} \pars{ t } \le \prod_{i=1}^{n} e^{p_i \pars{ e^{t}-1 } } = e^{ \pars{ e^{t}-1 } \mu}  .\]

	Ustalmy \(t > 0\), mamy \[P \pars{ X \ge \pars{ 1+\delta } \mu } = P \pars{ e ^{tX}\ge e^{t \pars{ 1+\delta } \mu} } \le \frac{\mathbb{E}\left[ e^{tX} \right] }{e^{t \pars{ 1+\delta } \mu}} \le  \frac{e^{\pars{e^{t}-1}\mu}}{e^{t \pars{ 1+\delta } \mu}} .\]
	Niech \(t = \ln \pars{ 1+\delta } > 0\). Wychodzi nam \(P \pars{ X \ge \pars{ 1+\delta } \mu } \le  \pars{\frac{e^{1+\delta-1}}{ \pars{ 1+\delta } ^{1+\delta}}} ^{\mu}\), co kończy dowód pierwszej części.

	Punkt drugi dowodzimy korzystając z pierwszego, wystarczy pokazać, że dla \(\delta \in (0,1]\) jest \[\frac{e^{\delta}}{ \pars{ 1+\delta } ^{1+\delta}} \le e^{- \frac{\delta^2}{3}}.\]

	Logarytmujemy stronami, chcemy pokazać, że \(\delta - \pars{ 1+ \delta }\ln \pars{ 1+\delta   } + \frac{\delta^2}{3} \le 0\). Oznaczmy lewą stronę przez \(f \pars{ \delta } \). Liczymy pochodne:
	\[f' \pars{ \delta } = 1- 1\cdot \ln \pars{ 1+\delta } - \frac{1+\delta}{1+\delta} + \frac{2}{3}\delta = - \ln \pars{ 1+\delta } + \frac{2}{3}\delta,\]
	\[f'' \pars{ \delta } = -\frac{1}{1+\delta} + \frac{2}{3}.\]

	\(f' \pars{ 0 } = 0\), a potem maleje do \(\delta = \frac{1}{2}\) (tam druga pochodna się zeruje, przedtem ujemna), potem rośnie, ale \(f' \pars{ 1 } < 0\), więc jest ujemna na całym \((0,1]\).

	\(f \pars{ \delta } \) tylko maleje na \((0,1]\), więc nierówność działa, bo \(f \pars{ 0 } = 0\).

	Dowodząc punkt trzeci zakładamy \(R \ge 6\mu\). Niech \(R = \pars{ 1+\delta } \mu\), czyli \(\delta = \frac{R}{\mu}-1 \ge 5\).

	\[P \pars{ X \ge \pars{ 1+\delta } \mu } \le \pars{ \frac{e^{\delta}}{ \pars{ 1+\delta } ^{1+\delta}} } ^{\mu} \le \pars{ \frac{e}{1+\delta} } ^{ \pars{ 1+\delta } \mu} \le \pars{ \frac{e}{6} } ^{R} \le \pars{ \frac{1}{2} } ^{R} = 2 ^{-R}.\]
\end{proof}
%</probabil-egzamin-chernoff-1>
%</probabil-2025-11-14-poisson-trial-chernoff-bound>

%<*probabil-2025-11-14-poisson-trial-chernoff-bound-2>
\begin{theorem}
	\label{poisson-trial-chernoff-lowerbounds}
	Niech \(X_1, \dots, X_n\) to niezależne próby Poissona. Dodatkowo oznaczamy \(X = \sum_{i=1}^n{X_i}\) i \(\mu = \expected{X}\). Wtedy dla każdego \(\delta \in (0,1)\) zachodzi
	\begin{enumerate}
		\item \( P \pars{ X \le \pars{ 1-\delta } \mu } \le \pars{ \frac{e^{-\delta}}{ \pars{ 1-\delta } ^{1-\delta}} } ^{\mu}\)
		\item \(P \pars{ X \le \pars{ 1-\delta } \mu } \le e^{-\frac{\mu\delta^2}{2}}\).
	\end{enumerate}
\end{theorem}
\begin{proof}
	Dowód identyczny jak w poprzednim twierdzeniu, wybieramy \(t = \ln \pars{ 1-\delta } < 0\) i korzystamy z tego, że \(e^{-z}\) jest antymonotoniczne. Drugiego punktu ponownie dowodzimy licząc pochodne i na ich podstawie dowodząc odpowiedniej nierówności.
\end{proof}
%</probabil-2025-11-14-poisson-trial-chernoff-bound-2>

\subsection{Przypadki specjalne}
\begin{theorem}
	Niech \(X_1,\ldots,X_n\) będą niezależnymi zmiennymi losowymi o rozkładzie prawdopodobieństwa \(P \pars{ X_i = 1 } = P \pars{ X_i = -1 } = \frac{1}{2}\). Niech \(X = \sum_{i=1}^{n} X_i\). Dla każdego \(a>0\) mamy \(P \pars{ X \ge a } \le e^{-\frac{a^2}{2n}}\). Zauważmy, że nie ma sensu rozważać tu odchyleń multiplikatywnych, bo \(\mathbb{E}\left[ X \right] = 0\).
\end{theorem}
\begin{proof}
	Mamy \(\mathbb{E}\left[ e^{tX_{i}} \right] = \frac{1}{2}e^{-t} + \frac{1}{2}e^{t}\).

	Rozwijamy w szereg Taylora:
	\[e^{t} = 1 + t + \frac{t^2}{2} + \ldots + \frac{t ^{i}}{i!} + \ldots\]
	\[e^{-t} = 1 -t + \frac{t^2}{2} + \ldots + \pars{ -1 } ^{i} \frac{t ^{i}}{i!} + \ldots\]

	Z tego wynika \[\mathbb{E}\left[ e^{tX_i} \right] = \sum_{i\ge 0}^{} \frac{t ^{2i}}{ \pars{ 2i } !} \le \sum_{i\ge 0}^{} \frac{ \pars{ \frac{t^2}{2} } ^{i}}{i!} = e^{\frac{t^2}{2}},\] gdzie w nierówności wyciągnęliśmy \(2\) z dwukrotności każdej liczby od \(1\) do \(i\), a pozostałe składniki zignorowaliśmy.

	Zatem \(\mathbb{E}\left[ e^{tX} \right] = \prod_{i=1}^{n} \mathbb{E}\left[e^{tX_i}\right] \le e^{\frac{t^2n}{2}} \).

	Dostajemy \(P \pars{ X\ge a } = P \pars{ e^{tX}\ge e^{ta} } \le \frac{\mathbb{E}\left[ e^{tX} \right] }{e^{ta}} \le e^{t^2n\cdot \frac{1}{2}-ta} = e^{a^2 \frac{1}{n} \frac{1}{2} - \frac{a^2}{n}} = e^{-a^2\cdot \frac{1}{2n}}\), gdzie podstawiliśmy \(t = \frac{a}{n} > 0\).
\end{proof}

\begin{theorem}
	Niech \(Y_1,\ldots,Y_n\) będą niezależnymi indykatorami \(P \pars{ Y_i = 0 } = P \pars{ Y_i = 1 } = \frac{1}{2}\). Niech \(Y = \sum_{i=1}^{n} Y_i\), \(\mu = \mathbb{E}\left[ Y \right] = \frac{n}{2}\). Wtedy
	\begin{enumerate}
		\item \(\forall_{a>0} \ P \pars{ Y \ge \mu+a } \le e^{-\frac{2a^2}{n}} \)
		\item \(\forall_{\delta > 0} \   P \pars{ Y \ge \pars{ 1+\delta } \mu } \le e ^{-\delta^2\mu}\)
	\end{enumerate}
\end{theorem}
\begin{proof}
	Bierzemy \(Y_i = \frac{X_i + 1}{2}\). Mamy \(P \pars{ X_i = 1 } = P \pars{ X_i = -1 } = \frac{1}{2}\). Dla  \(X = \sum_{i=1}^{n} X_i = 2Y-2\mu\) mamy \[ P \pars{ Y \ge \mu+a } = P \pars{ X \ge 2a } \le e^{-\frac{2a^2}{n}}  \]
	oraz \[ P \pars{ Y \ge \pars{ 1+\delta } \mu } = P \pars{ X \ge 2\delta\mu } \le e^{- \frac{2\delta^2\mu^2}{n}} = e^{-\delta^2\mu}. \]
\end{proof}

%<*probabil-egzamin-chernoff-2>
\subsection{Rzuty monetą}
%<*probabil-2025-10-24-nierownosc-chernoffa-2>
\begin{example}
	\label{chernoff-coin-tosses}
    Chcemy ograniczyć z góry prawdopodobieństwo, że przy \(n\) rzutach monetą wyrzucimy orła więcej niż \( \frac{3}{4} n \) razy.
%</probabil-egzamin-chernoff-2>

    Niech \( X_1, \ldots, X_n \) - ciąg niezależnych prób Poissona oraz \( \prob(X_i = 1) = p_i \). Niech \( \mu = \expected{X} = \sum_{i=1}^{n} p_i \). Niech \( \delta > 0 \) będzie ustalone.

    Liczymy z funkcji tworzącej:

    \[
        M_{X_i}(t) = \expected{e^{tX_i}} = p_ie^t + (1 - p_i) = 1 + p_i(e^t - 1) \leq e^{p_i(e^t-1)}
    \]

    Z niezależności:

    \[
        M_X(t) = \Pi_{i=1}^n M_{X_i}(t) \leq \Pi_{i=1}^n e^{p_i(e^t - 1)} = e^{(e^t - 1)\mu}
    \]

    Teraz stosujemy to dla monet:

    \[
        M_{X_i}(t) \leq e^{\frac{1}{2} \pars{ e^t - 1 }} \implies M_X(t) \leq e^{\frac{1}{2} n \pars{ e^t - 1 }}
    \]
    \[
        \prob\pars{ X \geq \frac{3}{4}n } \leq 
        \frac{e^{\frac{1}{2} n \pars{ e^t - 1 }}}{e^{\frac{3}{4} nt}} = e^{\frac{1}{2} n \pars{ e^t - 1 - \frac{3}{2} t }}
    \]

    Następnie minimalizujemy wykładnik:

    \[
        f(t) = e^t - 1 - \frac{3}{2}t \implies f'(t) = e^t - \frac{3}{2}
    \]

    Stąd mamy wartość minimalną \( f \) dla \( t = \ln\pars{\frac{3}{2}} \) równą \( -\frac{1}{10} \)

    \[
        \prob\pars{ X \geq \frac{3}{4}n } \leq e^{-\frac{n}{20}}
    \]
\end{example}
%</probabil-2025-10-24-nierownosc-chernoffa-2>

\begin{example}
	Możemy też ograniczyć prawdopodobieństwo że przy \(n\) rzutach monetą wyrzucimy orła więcej niż \( \frac{3}{4} n \) razy w inny sposób, po prostu stosując \ref{poisson-trial-chernoff-bounds}.
%<*probabil-egzamin-chernoff-3>
	Widzimy, że nasze rzuty to niezależne próby Poissona o \(p = \frac{1}{2}\). \(\mu = \frac{n}{2}, \delta = \frac{1}{2}\), a więc możemy użyć wzoru 2. W takim razie mamy
	\[
		\prob\pars{X \geq \frac{3}{4}n} \leq e^{\frac{-\mu\delta^2}{3}} = e^{\frac{-\frac{n}{2} \cdot \frac{1}{4}}{3}} = e^{\frac{-n}{24}}
	\]
\end{example}
%</probabil-egzamin-chernoff-3>

\subsection{Estymacja parametru}

\begin{definition}
	Mówimy, że \([\hat{p}-\delta , \hat{p}+\delta]\) jest \( \pars{ 1-\gamma } \) przedziałem ufności dla parametru \(p\), jeśli \(P \pars{ p \in [\hat{p}-\delta, \hat{p}+\delta] } \ge 1-\gamma\). Chcemy, żeby \(n,\gamma,\delta\) były małe, ale musi być między nimi jakiś balans.
\end{definition}

Bierzemy z dużej populacji próbkę wielkości \(n\) wybraną w sposób jednostajny. \(p\) to nieznana wartość -- szukane prawdopodobieństwo, które chcemy szacować (np. prawdopodobieństwo jakiejś mutacji genetycznej). Niech zmienna losowa \(X = \hat{p}n\) oznacza liczbę wystąpień tej mutacji w naszej próbce. Spodziewamy się, że jak \(n\) rośnie, to \(\hat{p} \to p\).

Jeśli \(p < \hat{p}-\delta\), to \(X = n\hat{p} > n \pars{ p+\delta } = np \cdot \pars{ 1+\frac{\delta}{p} } \) .

Jeśli \(p > \hat{p}+\delta\), to \(X = n \hat{p} < n \pars{ p-\delta } = np \cdot \pars{ 1-\frac{\delta}{p} } \).

Mamy \(\mathbb{E}\left[ X \right] = np\), a więc Czernow daje
\begin{align*}
	P \pars{ p \notin \left[ \hat{p}-\delta, \hat{p}+\delta \right]  } & = P \pars{ X < np \pars{ 1-\frac{\delta}{p} }  } + P \pars{ X > np \pars{ 1+\frac{\delta}{p} }  }                                                          \\
	                                                                         & \le 2\cdot e^{-np \cdot \pars{ \frac{\delta}{p} } ^2 \cdot \frac{1}{3}} = 2\cdot e ^{-n \frac{\delta^2}{p}\cdot \frac{1}{3}} \le 2\cdot e^{-n\frac{\delta^2}{3} } = \gamma .
\end{align*}
Pod koniec wzięliśmy \(p=1\), bo daje najgorsze ograniczenie. W ten sposób związaliśmy ze sobą wartości \(n,\gamma,\delta\).


\subsection{Problem Set Balancing}

Mamy macierz \(n\times m\) wypełnioną wartościami z \(\{0,1\} \).

\[ \begin{bmatrix} a_1 & \ldots & a_{1m} \\ \vdots & \ddots & \vdots \\ a_{n 1} & \ldots & a_{nm} \end{bmatrix}  \]

Każdy wiersz to jakaś cecha osoby, kolumna to osoba. Chcemy przeprowadzić jakieś badanie, a do tego potrzebujemy zrobić grupę badawczą i kontrolną, które będą możliwie identyczne (to znaczy o podobnym zagęszczeniu wszystkich cech).

Mnożymy tę macierz \(A\) przez wektor \(\overline{b} \in \{-1,1\} ^{m}\) (umieszczenie kolejnych osób w jednej lub drugiej grupie) i dostajemy wektor \(\overline{c}\), w którym będą różnice między ilością osób z daną cechą między grupami. Chcemy, żeby norma \(\left\|\overline{c}\right\|_{\infty}\) była jak najmniejsza.

Wektor \(\overline{b}\) wyznaczamy, losując.

\begin{theorem}
	Dla losowego \(\overline{b}\) (każda współrzędna niezależnie, jednostajnie z \(\{-1,1\} \)) zachodzi
	\[ P \pars{ \left\|A \overline{b}\right\|_{\infty}  \ge \sqrt{4m \ln n}  } \le \frac{2}{n}. \]
\end{theorem}
\begin{proof}
	Niech \(i\)-ty wiersz \(\overline{a_i} = a_{i 1}\ldots a_{im}\) ma w sobie \(k\) jedynek. \(Z = \sum_{j=1}^{m} a_{ij}b_j\) jest sumą \(k\) zmiennych losowych, które z równym prawdopodobieństwem przyjmują \(1\) i \(-1\).

	Mamy zatem \(P \pars{ \left|Z_i\right|\ge  \sqrt{4m \ln n}  } \le 2 e^{\frac{-4m \ln n}{2k}} \le \frac{2}{n^2}\), ostatnia nierówność wynika z \(m \ge k\). Jest to ograniczenie dla jednego wiersza, dla wszystkich wierszy dostajemy z union bounda ograniczenie \(\frac{2}{n}\).
\end{proof}

